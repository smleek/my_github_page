[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Welcome to my data science portfolio! This site shows my journey learning data science and analytics. Here you’ll find projects that demonstrate what I’ve learned and discovered.\n\n\nThis portfolio shows my work learning data science. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages.\n\n\n\n\nProgramming: Python, Pandas for data analysis\nVisualization: Creating charts with Matplotlib and Seaborn\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data\n\n\n\n\n\n\n\nLearn how I explore datasets to find interesting patterns and answer questions.\n\n\n\nSee how I gather data from different sources and prepare it for analysis.\n\n\n\nSee how I tackle a data science project beginning to end.\n\n\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "index.html#about-this-portfolio",
    "href": "index.html#about-this-portfolio",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "This portfolio shows my work learning data science. Each project includes:\n\nMy code with documentation\nVisualizations I created\nWhat I learned and discovered\n\nI built this site using Quarto and host it on GitHub Pages."
  },
  {
    "objectID": "index.html#skills-im-learning",
    "href": "index.html#skills-im-learning",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Programming: Python, Pandas for data analysis\nVisualization: Creating charts with Matplotlib and Seaborn\nData Collection: Getting data from files, websites, and APIs\nAnalysis: Finding patterns and answering questions with data"
  },
  {
    "objectID": "index.html#my-projects",
    "href": "index.html#my-projects",
    "title": "Welcome to My Data Science Portfolio",
    "section": "",
    "text": "Learn how I explore datasets to find interesting patterns and answer questions.\n\n\n\nSee how I gather data from different sources and prepare it for analysis.\n\n\n\nSee how I tackle a data science project beginning to end.\n\n\n\nThanks for visiting! Feel free to explore my projects and see what I’m learning."
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/index.html#all-projects",
    "href": "projects/index.html#all-projects",
    "title": "Projects Overview",
    "section": "",
    "text": "Description: Pick a dataset and explore it to discover insights and answer questions.\n\n\n\nDescription: Find an interesting data source, collect the data, and prepare it for analysis.\n\n\n\nDescription: A comprehensive project that shows off my data science skills.",
    "crumbs": [
      "Projects Overview"
    ]
  },
  {
    "objectID": "projects/eda.html",
    "href": "projects/eda.html",
    "title": "EDA Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "EDA Project"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "My name is Sujin Leek, and I am a junior studying statistics at Brigham Young University. I really enjoy math and its applications to science and real-world problems, which is how I got into data science. I hope to make a positive impact on the world through my career; I would like to work in research, but am open to diverse opportunities.\nI have internship experience at Lawrence Livermore National Laboratory, where I did statistical work (black-box optimization) for an equation of state project. I am currently working as a research assistant in the statistics department at BYU and will complete an internship at Oak Ridge National Laboratory this summer. Please feel free to contact me with any questions!\n\n\n\nThis is me"
  },
  {
    "objectID": "about.html#background",
    "href": "about.html#background",
    "title": "About Me",
    "section": "",
    "text": "My name is Sujin Leek, and I am a junior studying statistics at Brigham Young University. I really enjoy math and its applications to science and real-world problems, which is how I got into data science. I hope to make a positive impact on the world through my career; I would like to work in research, but am open to diverse opportunities.\nI have internship experience at Lawrence Livermore National Laboratory, where I did statistical work (black-box optimization) for an equation of state project. I am currently working as a research assistant in the statistics department at BYU and will complete an internship at Oak Ridge National Laboratory this summer. Please feel free to contact me with any questions!\n\n\n\nThis is me"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About Me",
    "section": "Education",
    "text": "Education\n\nStatistics: Emphasis Statistical Science, Minor Mathematics - Brigham Young University, April 2027\nRelevant Coursework: Introduction to Computer Science (Python), Applied R Programming, Probability and Inference I, II, Statistical Modeling I, II (ANOVA, Regression), Linear Algebra (with computational applications in Python), Calculus I, II, III, Data Science Process, Bayesian Statistics"
  },
  {
    "objectID": "about.html#skills-interests",
    "href": "about.html#skills-interests",
    "title": "About Me",
    "section": "Skills & Interests",
    "text": "Skills & Interests\n\nI have experience with Python and R - R for data analysis, and Python for scripting. I have not used Python for data analysis thus far, but am excited to work with Pandas and Numpy in my current courses.\nI grew up speaking Korean as a second language.\nOn my mission, I found out that I like teaching! I taught lots of English and Korean while I was serving in South Korea and was able to work as a teaching assistant when I returned to BYU.\nI love reading! I particularly enjoy books on science and public health, but like fiction as well. My favorite books that I’ve read recently are Empire of AI by Karen Hao and Blind Spots by Marty Makary.\n\n\nTechnical Skills\n\nProgramming: Python, R\nData Analysis: Pandas, NumPy, Tidyverse\nVisualization: Matplotlib, ggplot\nTools: Jupyter Notebooks, Git/GitHub\n\n\n\nAreas of Interest\nI am interested in all areas of science and love math, which is why I ended up in statistics. I enjoy learning new ways to work with data using computers. I did begin university in a public health major, so I am especially interested in epidemiology and statistical applications in biology, as well as social problems."
  },
  {
    "objectID": "about.html#goals",
    "href": "about.html#goals",
    "title": "About Me",
    "section": "Goals",
    "text": "Goals\nRight now, my focus is on becoming proficient in theory and tools such that I can answer questions I or others have about the world. I would like to continue doing research and learning new things for the rest of my life, and ideally, my chosen career path will allow me to have a positive impact on the world.\nAs I mentioned earlier, I enjoy learning about epidemiology and reading about social problems. Questions like “how can we leverage AI to help solve social problems?” and “are current methods of disease prevention statistically sound and realistically workable?” really appeal to me. I do, however, really enjoy math and proofs, so learning new methods in pure statistics is appealing to me as well."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About Me",
    "section": "Contact",
    "text": "Contact\n\nEmail: smleek04@gmail.com\nGitHub: github.com/smleek\nLinkedIn: linkedin.com/in/sujinleek\n\n\nThis portfolio showcases my learning progress and projects completed during my data science studies."
  },
  {
    "objectID": "projects/data-acquisition.html",
    "href": "projects/data-acquisition.html",
    "title": "Data Acquisition Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Data Acquisition Project"
    ]
  },
  {
    "objectID": "projects/final-project.html",
    "href": "projects/final-project.html",
    "title": "Final Project",
    "section": "",
    "text": "This is coming down the pipeline. Check again later.",
    "crumbs": [
      "Final Project"
    ]
  },
  {
    "objectID": "tutorial.html",
    "href": "tutorial.html",
    "title": "Tutorial Blog - February 20, 2026",
    "section": "",
    "text": "Random forest is a popular machine learning method used for classification and regression. Its advantage over logistic regression is that it doesn’t require fulfilled assumptions for normal distribution like “no outliers” and it does not assume linearity between the predictors and log-odds of the response, strict independence of errors, or absence of multicollinearity.\nWe will be using it here to classify patients as having diabetes or not. I was given this dataset as part of my Introduction to Regression course taken Fall 2025, where we used logistic regression to classify patients as having diabetes or not.\n\n\n\n“Students like me” means you know basic Python and have taken a regression class.\n\n\nStarting with imports: Since I’m most familiar with pandas, that’s what I’ll be using for this tutorial. We also need to import the scikit-learn package, which is a very popular Python package for data science.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nThe diabetes dataset is provided … somehow. The dataset is already pretty tidy, but I decided to drop the \"row\" column as I found it redundant. There isn’t any real cleaning to do.\ndiabetes = pd.read_csv(\"/path/to/Diabetes.txt\", sep = r\"\\s+\")\ndiabetes = diabetes.drop(columns = ['row'])\nAt this point, diabetes should output this:\n\n\n\ndiabetes dataset printout\n\n\n\n\n\nWe’re going to isolate our response variable and predictor variables to make life easier. (So far, this is just like logistic regression.)\npredictors = diabetes.drop(columns=['diabetes'])\nresponse = diabetes['diabetes']\nGood practice in machine learning is to split up data before training and evaluating a model for later model validation. Scikit-learn makes this really easy.\nHere, I’ve chosen the variable names pred_train, pred_test, response_train, and response_test for the output of the train_test_split function. The function returns what I’ve described with the variable names: training predictor variables, testing predictor variables, training response variables, and testing response variables.\nI’ve chosen a test size of 0.2 for an 80-20 split, which is pretty standard. random_state is the same thing as set.seed().\npred_train, pred_test, response_train, response_test = train_test_split(predictors, response, test_size=0.2, random_state=1)\n\n\n\nRandom forest is named that way because it builds a bunch of decision trees. One tree is trained on a single bootstrap sample (random, with-replacement sample), meaning it can be thought of as a mini-model that is overfitted and has high variance.\nEach tree splits data into regions based on feature thresholds. At each split, the tree only considers a random subset of predictors, which reduces correlation between trees.\nFor the diabetes example, a single tree might say that an individual who has been pregnant 5 times and has a glucose level about 150 is positive for type 2 diabetes, but ignore the patient’s BMI.\nEach tree is considered to create the overall model. In the case of classification, a majority vote is used. When we test the model, every tree (in this case, 100) will evaluate the predictor variables and output a 1 or a 0. The majority vote wins, classifying a patient as either having diabetes or not.\n\n\n\nNow it’s time to fit our model! n_estimators refers to the number of trees in the forest, and we’ve again set random_state = 1. Note that we train (fit) the model on the previously allocated 80% of the data.\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 1)\nrandom_forest.fit(pred_train, response_train)\nThis should output the below image, which summarizes parameters from the model.\n\n\n\n‘random forest classifier’\n\n\n\n\n\nThis will output an array of 1s and 0s, where 1 means YES! THE PATIENT HAS DIABETES! and 0 means NO, THE PATIENT DOES NOT HAVE DIABETES. Note that the predictions are done on the earlier sectioned-out pred_test subset of the data, where pred_test is 20% of the data.\npredictions = random_forest.predict(pred_test)\npredictions\n\n\n\nIf you set your random_state = 1, then the results should look identical to mine. If not, they’re probably pretty similar.\n\n\n\nmodel evaluation\n\n\nAccuracy is around 75%, which is certainly better than 50%, which is the baseline for deciding whether a classification model is good or not. Interestingly, the model I fit last semester using logistic regression was substantially better with an accuracy of 80.6%.\n\n\n\n\nSo, it turns out I picked not the greatest dataset to show off random forest with, since logistic regression seems to have done better than random forest. However, we know that random forest is more robust to overfitting (so, it’s possible that the random forest model is secretly better), and we can now try this new method on other datasets. It’s also important to note that random forest can classify into more than two bins. Try out random forest on a dataset you’re curious about!\n\nThis tutorial was created as an assignment for STAT 386."
  },
  {
    "objectID": "tutorial.html#introduction",
    "href": "tutorial.html#introduction",
    "title": "Tutorial Blog - February 20, 2026",
    "section": "",
    "text": "Random forest is a popular machine learning method used for classification and regression. Its advantage over logistic regression is that it doesn’t require fulfilled assumptions for normal distribution like “no outliers” and it does not assume linearity between the predictors and log-odds of the response, strict independence of errors, or absence of multicollinearity.\nWe will be using it here to classify patients as having diabetes or not. I was given this dataset as part of my Introduction to Regression course taken Fall 2025, where we used logistic regression to classify patients as having diabetes or not."
  },
  {
    "objectID": "tutorial.html#tutorial-aimed-at-students-like-me",
    "href": "tutorial.html#tutorial-aimed-at-students-like-me",
    "title": "Tutorial Blog - February 20, 2026",
    "section": "",
    "text": "“Students like me” means you know basic Python and have taken a regression class.\n\n\nStarting with imports: Since I’m most familiar with pandas, that’s what I’ll be using for this tutorial. We also need to import the scikit-learn package, which is a very popular Python package for data science.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nThe diabetes dataset is provided … somehow. The dataset is already pretty tidy, but I decided to drop the \"row\" column as I found it redundant. There isn’t any real cleaning to do.\ndiabetes = pd.read_csv(\"/path/to/Diabetes.txt\", sep = r\"\\s+\")\ndiabetes = diabetes.drop(columns = ['row'])\nAt this point, diabetes should output this:\n\n\n\ndiabetes dataset printout\n\n\n\n\n\nWe’re going to isolate our response variable and predictor variables to make life easier. (So far, this is just like logistic regression.)\npredictors = diabetes.drop(columns=['diabetes'])\nresponse = diabetes['diabetes']\nGood practice in machine learning is to split up data before training and evaluating a model for later model validation. Scikit-learn makes this really easy.\nHere, I’ve chosen the variable names pred_train, pred_test, response_train, and response_test for the output of the train_test_split function. The function returns what I’ve described with the variable names: training predictor variables, testing predictor variables, training response variables, and testing response variables.\nI’ve chosen a test size of 0.2 for an 80-20 split, which is pretty standard. random_state is the same thing as set.seed().\npred_train, pred_test, response_train, response_test = train_test_split(predictors, response, test_size=0.2, random_state=1)\n\n\n\nRandom forest is named that way because it builds a bunch of decision trees. One tree is trained on a single bootstrap sample (random, with-replacement sample), meaning it can be thought of as a mini-model that is overfitted and has high variance.\nEach tree splits data into regions based on feature thresholds. At each split, the tree only considers a random subset of predictors, which reduces correlation between trees.\nFor the diabetes example, a single tree might say that an individual who has been pregnant 5 times and has a glucose level about 150 is positive for type 2 diabetes, but ignore the patient’s BMI.\nEach tree is considered to create the overall model. In the case of classification, a majority vote is used. When we test the model, every tree (in this case, 100) will evaluate the predictor variables and output a 1 or a 0. The majority vote wins, classifying a patient as either having diabetes or not.\n\n\n\nNow it’s time to fit our model! n_estimators refers to the number of trees in the forest, and we’ve again set random_state = 1. Note that we train (fit) the model on the previously allocated 80% of the data.\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 1)\nrandom_forest.fit(pred_train, response_train)\nThis should output the below image, which summarizes parameters from the model.\n\n\n\n‘random forest classifier’\n\n\n\n\n\nThis will output an array of 1s and 0s, where 1 means YES! THE PATIENT HAS DIABETES! and 0 means NO, THE PATIENT DOES NOT HAVE DIABETES. Note that the predictions are done on the earlier sectioned-out pred_test subset of the data, where pred_test is 20% of the data.\npredictions = random_forest.predict(pred_test)\npredictions\n\n\n\nIf you set your random_state = 1, then the results should look identical to mine. If not, they’re probably pretty similar.\n\n\n\nmodel evaluation\n\n\nAccuracy is around 75%, which is certainly better than 50%, which is the baseline for deciding whether a classification model is good or not. Interestingly, the model I fit last semester using logistic regression was substantially better with an accuracy of 80.6%."
  },
  {
    "objectID": "tutorial.html#conclusion",
    "href": "tutorial.html#conclusion",
    "title": "Tutorial Blog - February 20, 2026",
    "section": "",
    "text": "So, it turns out I picked not the greatest dataset to show off random forest with, since logistic regression seems to have done better than random forest. However, we know that random forest is more robust to overfitting (so, it’s possible that the random forest model is secretly better), and we can now try this new method on other datasets. It’s also important to note that random forest can classify into more than two bins. Try out random forest on a dataset you’re curious about!\n\nThis tutorial was created as an assignment for STAT 386."
  },
  {
    "objectID": "projects/tutorial.html",
    "href": "projects/tutorial.html",
    "title": "Tutorial: Random Forest for Classification",
    "section": "",
    "text": "Random forest is a popular machine learning method used for classification and regression. Its advantage over logistic regression is that it doesn’t require fulfilled assumptions for normal distribution like “no outliers” and it does not assume linearity between the predictors and log-odds of the response, strict independence of errors, or absence of multicollinearity.\nWe will be using it here to classify patients as having diabetes or not. I was given this dataset as part of my Introduction to Regression course taken Fall 2025, where we used logistic regression to classify patients as having diabetes or not.\n\n\n\n“Students like me” means you know basic Python and have taken a regression class.\n\n\nStarting with imports: Since I’m most familiar with pandas, that’s what I’ll be using for this tutorial. We also need to import the scikit-learn package, which is a very popular Python package for data science.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nThe diabetes dataset is provided … somehow. The dataset is already pretty tidy, but I decided to drop the \"row\" column as I found it redundant. There isn’t any real cleaning to do.\ndiabetes = pd.read_csv(\"/path/to/Diabetes.txt\", sep = r\"\\s+\")\ndiabetes = diabetes.drop(columns = ['row'])\nAt this point, diabetes should output this:\n\n\n\ndiabetes dataset printout\n\n\n\n\n\nWe’re going to isolate our response variable and predictor variables to make life easier. (So far, this is just like logistic regression.)\npredictors = diabetes.drop(columns=['diabetes'])\nresponse = diabetes['diabetes']\nGood practice in machine learning is to split up data before training and evaluating a model for later model validation. Scikit-learn makes this really easy.\nHere, I’ve chosen the variable names pred_train, pred_test, response_train, and response_test for the output of the train_test_split function. The function returns what I’ve described with the variable names: training predictor variables, testing predictor variables, training response variables, and testing response variables.\nI’ve chosen a test size of 0.2 for an 80-20 split, which is pretty standard. random_state is the same thing as set.seed().\npred_train, pred_test, response_train, response_test = train_test_split(predictors, response, test_size=0.2, random_state=1)\n\n\n\nRandom forest is named that way because it builds a bunch of decision trees. One tree is trained on a single bootstrap sample (random, with-replacement sample), meaning it can be thought of as a mini-model that is overfitted and has high variance.\nEach tree splits data into regions based on feature thresholds. At each split, the tree only considers a random subset of predictors, which reduces correlation between trees.\nFor the diabetes example, a single tree might say that an individual who has been pregnant 5 times and has a glucose level about 150 is positive for type 2 diabetes, but ignore the patient’s BMI.\nEach tree is considered to create the overall model. In the case of classification, a majority vote is used. When we test the model, every tree (in this case, 100) will evaluate the predictor variables and output a 1 or a 0. The majority vote wins, classifying a patient as either having diabetes or not.\n\n\n\nNow it’s time to fit our model! n_estimators refers to the number of trees in the forest, and we’ve again set random_state = 1. Note that we train (fit) the model on the previously allocated 80% of the data.\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 1)\nrandom_forest.fit(pred_train, response_train)\nThis should output the below image, which summarizes parameters from the model.\n\n\n\n‘random forest classifier’\n\n\n\n\n\nThis will output an array of 1s and 0s, where 1 means YES! THE PATIENT HAS DIABETES! and 0 means NO, THE PATIENT DOES NOT HAVE DIABETES. Note that the predictions are done on the earlier sectioned-out pred_test subset of the data, where pred_test is 20% of the data.\npredictions = random_forest.predict(pred_test)\npredictions\n\n\n\nIf you set your random_state = 1, then the results should look identical to mine. If not, they’re probably pretty similar.\n\n\n\nmodel evaluation\n\n\nAccuracy is around 75%, which is certainly better than 50%, which is the baseline for deciding whether a classification model is good or not. Interestingly, the model I fit last semester using logistic regression was substantially better with an accuracy of 80.6%.\n\n\n\n\nSo, it turns out I picked not the greatest dataset to show off random forest with, since logistic regression seems to have done better than random forest. However, we know that random forest is more robust to overfitting (so, it’s possible that the random forest model is secretly better), and we can now try this new method on other datasets. It’s also important to note that random forest can classify into more than two bins. Try out random forest on a dataset you’re curious about!\n\nThis tutorial was created as an assignment for STAT 386.",
    "crumbs": [
      "Tutorial Blog - February 20, 2026"
    ]
  },
  {
    "objectID": "projects/tutorial.html#introduction",
    "href": "projects/tutorial.html#introduction",
    "title": "Tutorial: Random Forest for Classification",
    "section": "",
    "text": "Random forest is a popular machine learning method used for classification and regression. Its advantage over logistic regression is that it doesn’t require fulfilled assumptions for normal distribution like “no outliers” and it does not assume linearity between the predictors and log-odds of the response, strict independence of errors, or absence of multicollinearity.\nWe will be using it here to classify patients as having diabetes or not. I was given this dataset as part of my Introduction to Regression course taken Fall 2025, where we used logistic regression to classify patients as having diabetes or not.",
    "crumbs": [
      "Tutorial Blog - February 20, 2026"
    ]
  },
  {
    "objectID": "projects/tutorial.html#tutorial-aimed-at-students-like-me",
    "href": "projects/tutorial.html#tutorial-aimed-at-students-like-me",
    "title": "Tutorial: Random Forest for Classification",
    "section": "",
    "text": "“Students like me” means you know basic Python and have taken a regression class.\n\n\nStarting with imports: Since I’m most familiar with pandas, that’s what I’ll be using for this tutorial. We also need to import the scikit-learn package, which is a very popular Python package for data science.\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nThe diabetes dataset is provided … somehow. The dataset is already pretty tidy, but I decided to drop the \"row\" column as I found it redundant. There isn’t any real cleaning to do.\ndiabetes = pd.read_csv(\"/path/to/Diabetes.txt\", sep = r\"\\s+\")\ndiabetes = diabetes.drop(columns = ['row'])\nAt this point, diabetes should output this:\n\n\n\ndiabetes dataset printout\n\n\n\n\n\nWe’re going to isolate our response variable and predictor variables to make life easier. (So far, this is just like logistic regression.)\npredictors = diabetes.drop(columns=['diabetes'])\nresponse = diabetes['diabetes']\nGood practice in machine learning is to split up data before training and evaluating a model for later model validation. Scikit-learn makes this really easy.\nHere, I’ve chosen the variable names pred_train, pred_test, response_train, and response_test for the output of the train_test_split function. The function returns what I’ve described with the variable names: training predictor variables, testing predictor variables, training response variables, and testing response variables.\nI’ve chosen a test size of 0.2 for an 80-20 split, which is pretty standard. random_state is the same thing as set.seed().\npred_train, pred_test, response_train, response_test = train_test_split(predictors, response, test_size=0.2, random_state=1)\n\n\n\nRandom forest is named that way because it builds a bunch of decision trees. One tree is trained on a single bootstrap sample (random, with-replacement sample), meaning it can be thought of as a mini-model that is overfitted and has high variance.\nEach tree splits data into regions based on feature thresholds. At each split, the tree only considers a random subset of predictors, which reduces correlation between trees.\nFor the diabetes example, a single tree might say that an individual who has been pregnant 5 times and has a glucose level about 150 is positive for type 2 diabetes, but ignore the patient’s BMI.\nEach tree is considered to create the overall model. In the case of classification, a majority vote is used. When we test the model, every tree (in this case, 100) will evaluate the predictor variables and output a 1 or a 0. The majority vote wins, classifying a patient as either having diabetes or not.\n\n\n\nNow it’s time to fit our model! n_estimators refers to the number of trees in the forest, and we’ve again set random_state = 1. Note that we train (fit) the model on the previously allocated 80% of the data.\nrandom_forest = RandomForestClassifier(n_estimators = 100, random_state = 1)\nrandom_forest.fit(pred_train, response_train)\nThis should output the below image, which summarizes parameters from the model.\n\n\n\n‘random forest classifier’\n\n\n\n\n\nThis will output an array of 1s and 0s, where 1 means YES! THE PATIENT HAS DIABETES! and 0 means NO, THE PATIENT DOES NOT HAVE DIABETES. Note that the predictions are done on the earlier sectioned-out pred_test subset of the data, where pred_test is 20% of the data.\npredictions = random_forest.predict(pred_test)\npredictions\n\n\n\nIf you set your random_state = 1, then the results should look identical to mine. If not, they’re probably pretty similar.\n\n\n\nmodel evaluation\n\n\nAccuracy is around 75%, which is certainly better than 50%, which is the baseline for deciding whether a classification model is good or not. Interestingly, the model I fit last semester using logistic regression was substantially better with an accuracy of 80.6%.",
    "crumbs": [
      "Tutorial Blog - February 20, 2026"
    ]
  },
  {
    "objectID": "projects/tutorial.html#conclusion",
    "href": "projects/tutorial.html#conclusion",
    "title": "Tutorial: Random Forest for Classification",
    "section": "",
    "text": "So, it turns out I picked not the greatest dataset to show off random forest with, since logistic regression seems to have done better than random forest. However, we know that random forest is more robust to overfitting (so, it’s possible that the random forest model is secretly better), and we can now try this new method on other datasets. It’s also important to note that random forest can classify into more than two bins. Try out random forest on a dataset you’re curious about!\n\nThis tutorial was created as an assignment for STAT 386.",
    "crumbs": [
      "Tutorial Blog - February 20, 2026"
    ]
  }
]